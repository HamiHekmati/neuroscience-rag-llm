{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",

  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**\n",
        "\n",
        "This cell imports all the necessary libraries and modules required to build a retrieval-augmented question-answering pipeline for neuroscience literature. The imports include modules for data retrieval from external sources (arXiv, bioRxiv), text preprocessing, document embedding, and vector storage. Additionally, libraries are included for working with large language models (transformers, quantization), constructing retrieval and generation chains, and deploying the interactive web interface using Gradio. These components together enable the system to fetch scientific abstracts, process and embed them for semantic search, and generate reliable, context-aware answers using advanced language models."
      ],
      "metadata": {
        "id": "UzI23z-bWoZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import arxiv\n",
        "import requests\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_community.llms import HuggingFacePipeline\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from transformers import pipeline\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import gradio as gr"
      ],
      "metadata": {
        "id": "Jx9Wkb4z_7KH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Neuroscience Topics**\n",
        "\n",
        "This cell lists core topics in neuroscience and neuroengineering to guide literature searches. These keywords are used to retrieve relevant research abstracts from public databases, ensuring broad coverage of important areas in the field."
      ],
      "metadata": {
        "id": "rYiZlWV4XDwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define neuroscience topics\n",
        "NEURO_TOPICS = [\n",
        "    \"neuroscience\", \"neuroengineering\", \"brain-computer interface\",\n",
        "    \"neuroprosthetics\", \"cognitive neuroscience\", \"motor control\",\n",
        "    \"neuroimaging\", \"fMRI\", \"EEG\", \"neural decoding\", \"neural interfaces\",\n",
        "    \"computational neuroscience\", \"synaptic plasticity\", \"learning and memory\",\n",
        "    \"brain signal processing\", \"spiking neural networks\", \"neuromodulation\"\n",
        "]"
      ],
      "metadata": {
        "id": "-FWOZj87_-wf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clean Raw Text**\n",
        "\n",
        "To ensure consistent input for the retrieval pipeline, this function preprocesses and standardizes raw text by removing extra whitespace and unwanted characters. By cleaning and formatting the research abstracts, it improves the quality of embeddings and enhances the accuracy of document retrieval and language model responses."
      ],
      "metadata": {
        "id": "o-BT7NZ6YY2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean raw text\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s.,;:!?()-]', '', text)\n",
        "    return text.strip()\n"
      ],
      "metadata": {
        "id": "Jkvtva4BABm-"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fetch Abstracts from arXiv**\n",
        "\n",
        "This function automates the retrieval of research abstracts from the arXiv database based on a list of neuroscience-related topics. For each topic, it queries arXiv for recent papers, extracts the title and abstract, and applies text cleaning for consistency. The collected and processed abstracts are then returned as a list, forming the basis for downstream embedding and retrieval tasks."
      ],
      "metadata": {
        "id": "WMEUjB59ZrTP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch abstracts from arXiv\n",
        "def fetch_arxiv_abstracts(topics, max_per_topic=2):\n",
        "    abstracts = []\n",
        "    for topic in topics:\n",
        "        search = arxiv.Search(\n",
        "            query=topic,\n",
        "            max_results=max_per_topic,\n",
        "            sort_by=arxiv.SortCriterion.SubmittedDate\n",
        "        )\n",
        "        for result in search.results():\n",
        "            raw = f\"Title: {result.title}\\nAbstract: {result.summary}\"\n",
        "            abstracts.append(clean_text(raw))\n",
        "    return abstracts"
      ],
      "metadata": {
        "id": "gvaaBCtRAFuu"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bioRxiv Abstract Collection**\n",
        "\n",
        "To expand the literature base beyond arXiv, this function gathers recent neuroscience abstracts from the bioRxiv preprint server. It queries the bioRxiv API for each specified topic, extracts and cleans the title and abstract from each retrieved paper, and compiles the results into a list"
      ],
      "metadata": {
        "id": "bJjZzbMUaduj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch abstracts from bioRxiv\n",
        "def fetch_biorxiv_abstracts(topics, max_per_topic=2):\n",
        "    abstracts = []\n",
        "    for topic in topics:\n",
        "        url = f\"https://api.biorxiv.org/details/biorxiv/{topic}/0/{max_per_topic}\"\n",
        "        try:\n",
        "            res = requests.get(url).json()\n",
        "            for paper in res.get(\"collection\", []):\n",
        "                raw = f\"Title: {paper['title']}\\nAbstract: {paper['abstract']}\"\n",
        "                abstracts.append(clean_text(raw))\n",
        "        except Exception:\n",
        "            continue\n",
        "    return abstracts"
      ],
      "metadata": {
        "id": "CUr6BP47AIkC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fetch and Clean Abstracts**\n",
        "\n",
        "Retrieval and cleaning of neuroscience research abstracts from arXiv and bioRxiv are performed using the defined functions for each target topic. After gathering and processing the papers, the results are merged into a single dataset with consistent formatting. The total number of cleaned abstracts is displayed to verify that data aggregation has been completed successfully."
      ],
      "metadata": {
        "id": "tXhT5I9BbPMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fetch and clean abstracts\n",
        "print(\"ðŸ“˜ Fetching from arXiv...\")\n",
        "arxiv_docs = fetch_arxiv_abstracts(NEURO_TOPICS, max_per_topic=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1OOA4X2AMRr",
        "outputId": "5f64826a-af59-4a29-8adf-36a82ef5d16d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“˜ Fetching from arXiv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-266927979.py:10: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
            "  for result in search.results():\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ðŸ§ª Fetching from bioRxiv...\")\n",
        "biorxiv_docs = fetch_biorxiv_abstracts(NEURO_TOPICS, max_per_topic=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72lLmZV-AQGc",
        "outputId": "62815416-e22a-4682-a0d0-a87b9c79943a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ§ª Fetching from bioRxiv...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_cleaned_docs = arxiv_docs + biorxiv_docs\n",
        "print(f\"âœ… Total cleaned abstracts: {len(all_cleaned_docs)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpMMF7iuBOrx",
        "outputId": "044fdf86-9ca6-4830-e0b2-cee6ab705740"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Total cleaned abstracts: 1556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chunk the Cleaned Documents**\n",
        "\n",
        "To prepare the research abstracts for semantic search and retrieval, this section splits each cleaned document into smaller, overlapping text chunks. By breaking long texts into manageable segments, it ensures that relevant information can be efficiently indexed and retrieved during question answering. The chunking parameters are set to balance context and granularity for optimal embedding performance."
      ],
      "metadata": {
        "id": "qtcVUyUmbu1q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chunk the cleaned documents\n",
        "text_splitter = CharacterTextSplitter(\n",
        "    separator=\"\\n\",\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=100,\n",
        "    length_function=len\n",
        ")\n",
        "chunks = []\n",
        "for doc in all_cleaned_docs:\n",
        "    chunks.extend(text_splitter.split_text(doc))"
      ],
      "metadata": {
        "id": "ixm5ab-kBS9r"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Embed and Store Using Chroma**\n",
        "\n",
        "By converting each text chunk into a vector representation with Hugging Face embeddings, the workflow enables efficient similarity search over the dataset. These embeddings are saved in a Chroma vector database, and a retriever is set up to rapidly find and return the most relevant document sections during question answering."
      ],
      "metadata": {
        "id": "irG6D-Fpce2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed and store using Chroma\n",
        "embedding = HuggingFaceEmbeddings()\n",
        "vectorstore = Chroma.from_texts(chunks, embedding)\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460,
        },
        "id": "0W29vdDLB7N8",
        "outputId": "ce7029dc-3b06-4e73-cbb1-2713cb5dc2d2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-685671482.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embedding = HuggingFaceEmbeddings()\n",
            "/tmp/ipython-input-685671482.py:2: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
            "  embedding = HuggingFaceEmbeddings()\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1040aeada1ae439cbe4d080a79d25923"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "478ccbac9e4f41b898fc8d5aa995b01f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6803b6c3ed934a0fbeff4af0639daf03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0bf6e96fa5534fff806c5cde9726bf97"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e18d2ed90fa3406fb2e37020e92b1b7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0be2ae07f7c452993b36a54d83c5194"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc9252c13b2b45919882e63e04eca7b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e68f804846e145ca8ee34af2ccd41e9e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "474e4475a70b43d284e6e13751520690"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87dcad1ba30b41dfbc65ef5b7c5faa8b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27498558a7a446df9416f519c003f4b7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Configure the Language Model**\n",
        "\n",
        "This cell loads the Mistral-7B-Instruct-v0.2 large language model in 4-bit quantized mode to optimize memory efficiency on GPU hardware. The BitsAndBytesConfig enables efficient quantization, while the Hugging Face Transformers pipeline is used to build a text-generation interface around the model. After loading the tokenizer and configuring the generation parameters, a sample prompt is used to verify the modelâ€™s ability to answer neuroscience questions, demonstrating the LLMâ€™s knowledge about the thalamus and its functions."
      ],
      "metadata": {
        "id": "JniaozsTdKPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Hugging Face model (Mistral-7B-Instruct-v0.2 in 4-bit quantization)\n",
        "print(\"ðŸ“¥ Loading Hugging Face model: mistralai/Mistral-7B-Instruct-v0.2 in 4-bit quantization\")\n",
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "\n",
        "# Load model and tokenizer\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto'\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create the pipeline\n",
        "hf_pipeline = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.0, # Keep temperature at 0 for greedy decoding\n",
        "    do_sample=False,\n",
        ")\n",
        "llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
        "\n",
        "# Test it\n",
        "response = llm(\"What are the roles of the thalamus in the brain?\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633,
          
        },
        "id": "1bWB9h_vCd0b",
        "outputId": "287d61d4-4f0c-44b0-ab29-6d3ef39c2a68"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“¥ Loading Hugging Face model: mistralai/Mistral-7B-Instruct-v0.2 in 4-bit quantization\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/596 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "12dadf06605d4ad3a989dfdc9e4a0ffc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/25.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25c58991bea3481d97bdb75e9fcff5db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac056593c9ac4976ba816e8e4740f545"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0856709e2c154ed28d75216949cf9547"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed85f62765fb4603a40410f6723c4f13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00003.safetensors:   0%|          | 0.00/4.54G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae91d3e5f58e418388e221047a4cad24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00a94a4f1eb444f89cfbd7d8dcb8fa45"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4dc2d76eec474a9987dc7a6074fc1cb7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/2.10k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91218f571bcb4c28a455e8b62e1b0591"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.model:   0%|          | 0.00/493k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7a559809ec8471a8377e164f0381242"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.80M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1a8841afb71485388ca2f6386696ad5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5758fb6d75d046e0a716fe121c2706d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "/tmp/ipython-input-2902563992.py:31: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
            "  llm = HuggingFacePipeline(pipeline=hf_pipeline)\n",
            "/tmp/ipython-input-2902563992.py:34: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = llm(\"What are the roles of the thalamus in the brain?\")\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What are the roles of the thalamus in the brain?\n",
            "\n",
            "The thalamus is a large, complex structure located in the midbrain that plays a crucial role in the processing and relaying of sensory information to the cerebral cortex. It acts as a relay station for sensory information from various parts of the body, including the senses of touch, taste, sight, hearing, and smell. The thalamus also plays a role in motor control, memory, attention, and consciousness. It receives information from the sensory organs and sends it to the appropriate areas of the cerebral cortex for further processing. Additionally, the thalamus plays a role in regulating the sleep-wake cycle and in the coordination of motor movements. Damage to the thalamus can result in various neurological symptoms, including sensory deficits, motor impairments, and cognitive difficulties.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build RAG QA Chain**\n",
        "\n",
        "To enable retrieval-augmented question answering, a RetrievalQA chain is constructed by combining the language model with the document retriever. This integration allows the system to answer queries using both the semantic capabilities of the LLM and relevant content extracted from the embedded neuroscience literature, ensuring responses are accurate and well-grounded in scientific sources."
      ],
      "metadata": {
        "id": "AbiOYpqSeDF_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build RAG QA chain\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever\n",
        ")"
      ],
      "metadata": {
        "id": "gMtYxkZ_Dr8r"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ask a Sample Question and Compare Answers**\n",
        "\n",
        "A neuroscience-related question is submitted to both the retrieval-augmented generation (RAG) pipeline and the language model alone. The RAG chain answers the question using relevant content extracted from the most recent neuroscience literature, while the direct LLM response relies only on pre-trained knowledge. Comparing the two outputs shows that the RAG approach provides more current and contextually grounded information from recent papers, whereas the direct LLM response delivers accurate but less up-to-date insights based on general model knowledge."
      ],
      "metadata": {
        "id": "zpeYhswqfaJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ask a sample question\n",
        "query = \"What do recent neuroscience papers say about brain-computer interfaces?\"\n",
        "print(\"\\nðŸ“Œ Question:\", query)\n",
        "\n",
        "# RAG answer (with context)\n",
        "rag_answer = qa.run(query)\n",
        "print(\"\\nðŸ§  Answer WITH RAG (using document retrieval):\")\n",
        "print(rag_answer)\n",
        "\n",
        "# Direct LLM answer (no documents)\n",
        "prompt = PromptTemplate.from_template(\"Answer the following question clearly:\\n{question}\")\n",
        "llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
        "direct_answer = llm_chain.run(query)\n",
        "print(\"\\nðŸ¤– Answer WITHOUT RAG (no document context):\")\n",
        "print(direct_answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQC7b-9PVbtG",
        "outputId": "f47140e7-6333-4258-8bf2-69672561994d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ“Œ Question: What do recent neuroscience papers say about brain-computer interfaces?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2428311421.py:6: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  rag_answer = qa.run(query)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
            "/tmp/ipython-input-2428311421.py:12: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  llm_chain = LLMChain(llm=llm, prompt=prompt)\n",
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸ§  Answer WITH RAG (using document retrieval):\n",
            "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
            "\n",
            "Title: On using AI for EEG-based BCI applications: problems, current challenges and future trends Abstract: Imagine unlocking the power of the mind to communicate, create, and even interact with the world around us. Recent breakthroughs in Artificial Intelligence (AI), especially in how machines see and understand language, are now fueling exciting progress in decoding brain signals from scalp electroencephalography (EEG). Prima facie, this opens the door to revolutionary brain-computer interfaces (BCIs) designed for real life, moving beyond traditional uses to envision Brain-to-Speech, Brain-to-Image, and even a Brain-to-Internet of Things (BCIoT). However, the journey is not as straightforward as it was for Computer Vision (CV) and Natural Language Processing (NLP). Applying AI to real-world EEG-based BCIs, particularly in building powerful foundational models, presents unique and intricate hurdles that could affect their reliability. Here, we unfold a guided exploration of this dynamic and rapidly evolving research area. Rather than barely outlining a map of current endeavors and results, the goal is to provide a principled navigation of this hot and cutting-edge research landscape. We consider the basic paradigms that emerge from a causal perspective and the attendant challenges presented to AI-based models. Looking ahead, we then discuss promising research avenues that could overcome todays technological, methodological, and ethical limitations. Our aim is to lay out a clear roadmap for creating truly practical and effective EEG-based BCI solutions that can thrive in everyday environments.\n",
            "\n",
            "Title: AdaBrain-Bench: Benchmarking Brain Foundation Models for Brain-Computer Interface Applications Abstract: Non-invasive Brain-Computer Interfaces (BCI) offer a safe and accessible means of connecting the human brain to external devices, with broad applications in home and clinical settings to enhance human capabilities. However, the high noise level and limited task-specific data in non-invasive signals constrain decoding capabilities. Recently, the adoption of self-supervised pre-training is transforming the landscape of non-invasive BCI research, enabling the development of brain foundation models to capture generic neural representations from large-scale unlabeled electroencephalography (EEG) signals with substantial noises. However, despite these advances, the field currently lacks comprehensive, practical and extensible benchmarks to assess the utility of the public foundation models across diverse BCI tasks, hindering their widespread adoption. To address this challenge, we present AdaBrain-Bench, a large-scale standardized benchmark to systematically evaluate brain foundation models in widespread non-invasive BCI tasks. AdaBrain-Bench encompasses a diverse collection of representative BCI decoding datasets spanning 7 key applications. It introduces a streamlined task adaptation pipeline integrated with multi-dimensional evaluation metrics and a set of adaptation tools. The benchmark delivers an inclusive framework for assessing generalizability of brain foundation models across key transfer settings, including cross-subject, multi-subject, and few-shot scenarios. We leverage AdaBrain-Bench to evaluate a suite of publicly available brain foundation models and offer insights into practices for selecting appropriate models in various scenarios. We make our benchmark pipeline available to enable reproducible research and external use, offering a continuously evolving platform to foster progress toward robust and generalized neural decoding solutions.\n",
            "\n",
            "Title: Towards Neural Co-Processors for the Brain: Combining Decoding and Encoding in Brain-Computer Interfaces Abstract: The field of brain-computer interfaces is poised to advance from the traditional goal of controlling prosthetic devices using brain signals to combining neural decoding and encoding within a single neuroprosthetic device. Such a device acts as a co-processor for the brain, with applications ranging from inducing Hebbian plasticity for rehabilitation after brain injury to reanimating paralyzed limbs and enhancing memory. We review recent progress in simultaneous decoding and encoding for closed-loop control and plasticity induction. To address the challenge of multi-channel decoding and encoding, we introduce a unifying framework for developing brain co-processors based on artificial neural networks and deep learning. These neural co-processors can be used to jointly optimize cost functions with the nervous system to achieve desired behaviors ranging from targeted neuro-rehabilitation to augmentation of brain function.\n",
            "\n",
            "Title: A Dynamical Cartography of the Epistemic Diffusion of Artificial Intelligence in Neuroscience Abstract: Neuroscience and AI have an intertwined history, largely relayed in the literature of both fields. In recent years, due to the engineering orientations of AI research and the monopoly of industry for its large-scale applications, the mutual expansion of neuroscience and AI in fundamental research seems challenged. In this paper, we bring some empirical evidences that, on the contrary, AI and neuroscience are continuing to grow together, but with a pronounced interest in the fields of study related to neurodegenerative diseases since the 1990s. With a temporal knowledge cartography of neuroscience drawn with advanced document embedding techniques, we draw the dynamical shaping of the discipline since the 1970s and identified the conceptual articulation of AI with this particular subfield mentioned before. However, a further analysis of the underlying citation network of the studied corpus shows that the produced AI technologies remain confined in the different subfields and are not transferred from one subfield to another. This invites us to discuss the genericity capability of AI in the context of an intradisciplinary development, especially in the diffusion of its associated metrology.\n",
            "\n",
            "Question: What do recent neuroscience papers say about brain-computer interfaces?\n",
            "Helpful Answer: Recent neuroscience papers discuss the advancements in brain-computer interfaces (BCIs), focusing on the use of artificial intelligence (AI) for EEG-based BCIs, benchmarking brain foundation models, and the development of neural co-processors. These advancements aim to improve the reliability, generalizability, and effectiveness of BCIs for various applications, including rehabilitation, controlling prosthetic devices, and enhancing memory. However, there are challenges in applying AI to real-world EEG-based BCIs, and the field lacks comprehensive benchmarks to assess the utility of public foundation models. Additionally, there is a need to develop neural co-processors that can jointly optimize decoding and encoding for closed-loop control and plasticity induction. The use of AI in neuroscience is continuing to grow, but the produced technologies remain confined within specific subfields and are not easily transferred between them.\n",
            "\n",
            "ðŸ¤– Answer WITHOUT RAG (no document context):\n",
            "Answer the following question clearly:\n",
            "What do recent neuroscience papers say about brain-computer interfaces?\n",
            "\n",
            "Recent neuroscience papers suggest that brain-computer interfaces (BCIs) have shown promising results in various applications, including restoring motor function, enhancing cognitive abilities, and improving communication for individuals with neurological disorders or paralysis.\n",
            "\n",
            "One study published in Nature Neuroscience in 2021 reported that a BCI system was able to decode motor intentions from the brain activity of paralyzed individuals and translate those intentions into control signals for a robotic arm, allowing them to perform tasks such as grasping objects.\n",
            "\n",
            "Another study published in the Journal of Neuroscience in 2020 demonstrated that BCIs could be used to enhance working memory capacity in healthy individuals. The study involved training participants to use a BCI system to manipulate a virtual object, which led to improvements in their working memory performance.\n",
            "\n",
            "Additionally, a review paper published in the journal Nature Reviews Neuroscience in 2021 discussed the potential of BCIs for enhancing communication abilities for individuals with speech disorders or paralysis. The paper highlighted various approaches to developing BCIs for communication, including decoding neural activity to generate speech or using brain signals to control virtual avatars for social interaction.\n",
            "\n",
            "Overall, these studies and reviews suggest that BCIs have the potential to revolutionize the field of neuroscience and offer significant benefits for individuals with neurological disorders or paralysis, as well as for healthy individuals seeking to enhance their cognitive abilities. However, challenges remain in improving the accuracy and reliability of BCIs, as well as addressing ethical concerns related to privacy and consent.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deploy the Gradio Interface**\n",
        "\n",
        "A Gradio function is defined to process user questions using the RAG pipeline and return contextually grounded answers. This function is then integrated into a user-friendly Gradio web interface, allowing users to interactively submit neuroscience queries and receive real-time responses based on the latest scientific literature from arXiv and bioRxiv. The resulting application provides an accessible and reliable tool for exploring neuroscience research through retrieval-augmented language model outputs."
      ],
      "metadata": {
        "id": "I6duyEfrf7D6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio function\n",
        "def answer(user_query):\n",
        "    rag = qa.run(user_query)\n",
        "    return rag\n",
        "\n",
        "# Launch Gradio UI\n",
        "gr.Interface(\n",
        "    fn=answer,\n",
        "    inputs=gr.Textbox(lines=3, placeholder=\"Ask a neuroscience question...\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"ðŸ“š Answer WITH RAG (arXiv + bioRxiv)\"),\n",
        "    ],\n",
        "    title=\"ðŸ§  Neuroscience Research Assistant\",\n",
        "    description=\"LLM enhanced with real scientific literature from arXiv and bioRxiv to deliver reliable answers in neuroscience.\"\n",
        ").launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        },
        "id": "YAnmdCMoYWr0",
        "outputId": "fe75b8b7-a9a2-4458-d3a7-992b86518759"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://528f1d859f4a7de367.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://528f1d859f4a7de367.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}
